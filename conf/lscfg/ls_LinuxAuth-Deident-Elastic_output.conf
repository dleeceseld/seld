# Author Doug Leece  dleece @ firstfiretech dot ca 
# Dec 17/2022 Inital POC configuration to develop parsers and deidentification modules
# Jan 04/2022 Converted field types to Elastic Common Schema
# Jan 08/2022 Changing to source input of deidentified file, output to Elastic
#
# Data Validation config only
# This logstash filter will ingest a log file and write to an Elasticsearch server, 
# The log files should be original or deidentified versions with the index name indicating source
# 
# Ubuntu Linux Auth Log ingestion and field parsing configuration file for Logstash, 
# Linux auth.log contains many events typically required for cyber security investigation
# providing good utility almost immediatly.
#
# This configuration uses the Elastic/Logstash file input plugin, making it suitable for 
# deidentifiying a log from a Linux host before sending to a third party for analysis and could
# be easily extended to analyze logs from a centralized syslog collector.
#
# Currently the path is hardcoded but can uses a wildcard after "auth.log"  to enable ingestiong 
# of any authlogs in the defined directory, allowing from different systems and different
# naming strategies (example below). Intended for use on local auth.log rather than 
# log events collected via syslog forwarding. 
#
#  eg   auth.log-lindev28_20221217 , auth.log-Oracle-QA_Wednesday

# Built using GROK patterns published by Elasticsearch
# https://github.com/logstash-plugins/logstash-patterns-core/blob/main/patterns/ecs-v1/grok-patterns
#
# Extracted field names aligned to Elastic Common Schema
# https://www.elastic.co/guide/en/ecs/current/ecs-field-reference.html



# Added for repeat testing,  may not be needed for release ()
input {
        file {
                path => "/opt/seld/deidentlogs/deident-linuxauth.log*"
                mode => "read"
                file_completed_action => "log"
                file_completed_log_path => "/opt/seld/tmp/authlog_processing.log"
                exit_after_read => true
                sincedb_path => "/opt/seld/tmp/authlog_tracker"
                sincedb_clean_after => 0.00135
        }
} # End input

# Filtering steps, note requirement for custom pattern files 
# Extract program type
filter{
    grok{
        match => { 'message' => '^%{SYSLOGTIMESTAMP:syslogts}%{SPACE}%{IPORHOST:host_name}%{SPACE}%{SYSLOGPROG}:%{GREEDYDATA:[log][syslog][msg]}' }
    }
}

# Alternate regex for deidentified hostnames, some of the characters used in FPE don't work with the default pattern
filter {
  if "_grokparsefailure" in [tags]{
    grok{
      match => { 'message' => '^%{SYSLOGTIMESTAMP:syslogts}%{SPACE}(?<host_name_deident>[_0-9A-Za-z\.-]*?)%{SPACE}%{SYSLOGPROG}:%{GREEDYDATA:[log][syslog][msg]}' }
    }
  }
  if [host_name_deident] {
    mutate { add_tag => "seld_eventsourcehostname"}
    mutate { remove_tag => ["_grokparsefailure"]}
  }
}

# Add tagging based on program name in the log,  this allows filtering of unidentified programs to make sure only log sources that are 
# deidentifiable are included in the output
filter {
  if [process][name] == "sudo" {
    mutate { add_tag => "authlog_sudo"}
  } else if [process][name] == "su" {
    mutate { add_tag => "authlog_sudo"}
  } else if [process][name] == "sshd" {
    mutate { add_tag => "authlog_sshd"}
  } else if [process][name] == "unix_chkpwd" {
    mutate { add_tag => "authlog_sshd"}
  } else if [process][name] == "systemd-logind" {
    mutate { add_tag => "authlog_logind"}
  } else if [process][name] == "login" {
    mutate { add_tag => "authlog_logind"}
  } else if [process][name] == "systemd" {
    mutate { add_tag => "authlog_systemd"}
  } else if [process][name] == "passwd"{
    mutate { add_tag => "authlog_account"}
  } else if [process][name] == "useradd"{
    mutate { add_tag => "authlog_account"}
  } else if [process][name] == "usermod"{
    mutate { add_tag => "authlog_account"}
  } else if [process][name] == "groupadd"{
    mutate { add_tag => "authlog_account"}
  } else if [process][name] == "groupmod"{
    mutate { add_tag => "authlog_account"}
  } else if [process][name] == "chage"{
    mutate { add_tag => "authlog_account"}
  } else if [process][name] == "chfn"{
    mutate { add_tag => "authlog_account"}
  } else if [process][name] == "CRON"{
    mutate { add_tag => "authlog_cron"}
  } else {
    mutate { add_tag => "authlog_unprocessed"}
  }
}

# Fix the timestamp to show the event timestamp, not the time it was imported into Elasticsearch, 
# requires the timestamp meta data be overwritten and all variations potentially observed in the log to be included in the match
filter {
  date{
    match => ["syslogts", "MMM dd HH:mm:ss", "MMM  d HH:mm:ss"]
    timezone => "America/Edmonton"
    target => "@timestamp"
    add_tag => "timestamp_original"
  }
}


# Elastic ECS specific fields (meta data)
filter{
  mutate { add_field => { "[event][kind]" => "event"}}
  # copy timestamp into event created
  mutate {add_field => {"[event][created]" => "%{@timestamp}"}}
}  # End collection artifact filter

# Modify or remove collection artifacts: 
# Overwrite the hostname from the log processor with the firewall hostname in log source, avoids confusion and 
# Logstash creates an array if there are more than one field with the same name, this would mess up searching and visualizations
filter {
  if [host_name] {
    mutate{ rename =>{ "host_name" => "[host][name]" }  }
    mutate { remove_field => ["host_name"] }
  }
  if [host_name_deident] {
    mutate{ rename =>{ "host_name_deident" => "[host][name]" }  }
    mutate { remove_field => ["host_name_deident"] }
  }
} # End hostname clean up


###################  Third Party Data Processor Extraction & Enrichment ####################################
#
#  Example below is a basic extraction of IP information from the logdata field, no error handling 
#  so ensure Elastic indexing ignores malformed IP addresses. Based on tagging and program names much more
#  advanced extraction and field categorization could be performed. 

filter{
  # Ignore the grok parsing errors because lots of records may not include an IP, overwritting to allow other warnings through
  grok{
      tag_on_failure => ["ip_address_notobserved"] 
      match => {'[log][syslog][msg]' => '^%{GREEDYDATA}%{IP:[source][ip]}%{GREEDYDATA}'}  
  }
  if [source][ip]{
    mutate { add_tag => "ip_address_observed"}
  }
} 




###################################################################################################################################
output {

# Comment out if debugging not required ( improves performance marginally and but still allows logstash stdout events to be observed)
  stdout { codec => rubydebug }

################################################## File output section #############################################################

  # unprocessed logs identifes events for which no program based conditional processing identified. 
  # Additional general tags like host_name may still result in a very basic deidentification taking place but the contents 
  # of this file should be reviewed to see what sensitive data may warrant parser development or dropping the events
  if "authlog_unprocessed" in [tags] {
    file { path => "/opt/seld/debuglogs/linuxauth-unprocessed.log" 
      codec => line { format => '%{[message]}' }      
    }
  }

## Uncomment to enable persistance in an elastic search instance of the original data
 elasticsearch {
    hosts => ["Your_Elastic_host:9200"]
    index => "Your_Elastic_Indexname"
    cacert => "/etc/logstash/Your_Elastic_host.crt"
    ssl_certificate_verification => false
    ssl => true
    user => "elastic"
    password => "your_Elastic_Secret"
  }


# End of output filter
}